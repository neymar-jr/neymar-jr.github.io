<!DOCTYPE html>
<html lang="zh-CN" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>语义分割总结 - 刘宣辰的博客</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="刘宣辰" />
  <meta name="description" content="全自动的彩色眼底图像分割是自动化的眼底疾病筛查系统和辅助诊断系统的基础，能大大改善传统眼底疾病筛查和诊断的流程，降低眼科医生的阅片的工作量，" />

  <meta name="keywords" content="CS, ML, DL" />






<meta name="generator" content="Hugo 0.80.0" />


<link rel="canonical" href="https://neymar-jr.github.io/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%80%BB%E7%BB%93/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.f1e506a781bf25d33ffc18aa6b4e972a965c58049d27d4f92b7db2e9bf28e4bf.css" integrity="sha256-8eUGp4G/JdM//Biqa06XKpZcWASdJ9T5K32y6b8o5L8=" media="screen" crossorigin="anonymous">





<meta property="og:title" content="语义分割总结" />
<meta property="og:description" content="全自动的彩色眼底图像分割是自动化的眼底疾病筛查系统和辅助诊断系统的基础，能大大改善传统眼底疾病筛查和诊断的流程，降低眼科医生的阅片的工作量，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://neymar-jr.github.io/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%80%BB%E7%BB%93/" />
<meta property="article:published_time" content="2021-03-03T16:03:59+08:00" />
<meta property="article:modified_time" content="2021-03-03T16:03:59+08:00" />
<meta itemprop="name" content="语义分割总结">
<meta itemprop="description" content="全自动的彩色眼底图像分割是自动化的眼底疾病筛查系统和辅助诊断系统的基础，能大大改善传统眼底疾病筛查和诊断的流程，降低眼科医生的阅片的工作量，">
<meta itemprop="datePublished" content="2021-03-03T16:03:59+08:00" />
<meta itemprop="dateModified" content="2021-03-03T16:03:59+08:00" />
<meta itemprop="wordCount" content="12043">



<meta itemprop="keywords" content="语义分割," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="语义分割总结"/>
<meta name="twitter:description" content="全自动的彩色眼底图像分割是自动化的眼底疾病筛查系统和辅助诊断系统的基础，能大大改善传统眼底疾病筛查和诊断的流程，降低眼科医生的阅片的工作量，"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jane</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/">Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/posts/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/categories/">Categories</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/about/">About</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://github.com/neymar-jr" rel="noopener" target="_blank">
              external-link
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  
    <style>
      .fork-me-on-github {
        position: absolute; top: 0; left: 0; border: 0;
      }
      @media (max-width: 1080px) {
        .fork-me-on-github {
          display: none;
        }
      }
    </style>

    <a href="https://github.com/neymar-jr">
      <img class="fork-me-on-github"
        src="https://s3.amazonaws.com/github/ribbons/forkme_left_red_aa0000.png"
        alt="Fork me on GitHub">
    </a>
  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      Jane
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/">Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/posts/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/categories/">Categories</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://neymar-jr.github.io/about/">About</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://github.com/neymar-jr" rel="noopener" target="_blank">
              external-link
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
  
  <header class="post-header">
    <h1 class="post-title">语义分割总结</h1>
    
    <div class="post-meta">
      <time datetime="2021-03-03" class="post-time">
        2021-03-03
      </time>
    </div>
  </header>

  
  
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Table of Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#研究背景与意义">研究背景与意义</a></li>
    <li><a href="#语义分割方法总结">语义分割方法总结</a></li>
  </ul>

  <ul>
    <li><a href="#视杯和视盘分割">视杯和视盘分割</a></li>
    <li><a href="#血管分割">血管分割</a></li>
  </ul>

  <ul>
    <li><a href="#术语及背景知识">术语及背景知识</a>
      <ul>
        <li><a href="#语义分割">语义分割</a></li>
        <li><a href="#卷积神经网络">卷积神经网络</a></li>
        <li><a href="#rnn和lstm">RNN和LSTM</a></li>
        <li><a href="#编码器-解码器和自编码器模型">编码器-解码器和自编码器模型</a></li>
        <li><a href="#transformer">Transformer</a></li>
        <li><a href="#迁移学习">迁移学习</a></li>
      </ul>
    </li>
    <li><a href="#数据集">数据集</a>
      <ul>
        <li><a href="#starestructured-snalysis-of-the-retinal">STARE(structured snalysis of the retinal)</a></li>
        <li><a href="#drivedigital-retinal-images-for-vessel-extraction">DRIVE(digital retinal images for vessel extraction)</a></li>
        <li><a href="#idridindian-diabetic-retinopathy-image-dataset">IDRiD(Indian Diabetic Retinopathy Image Dataset)</a></li>
        <li><a href="#oiaophthalmic-image-analysis">OIA(Ophthalmic Image Analysis)</a></li>
        <li><a href="#refugeretinal-fundus-glaucoma-challenge">REFUGE(Retinal Fundus Glaucoma Challenge)</a></li>
      </ul>
    </li>
    <li><a href="#评价指标">评价指标</a>
      <ul>
        <li><a href="#pixel-accuracy-pa">Pixel Accuracy (PA)</a></li>
        <li><a href="#mean-pixel-accuracy-mpa">Mean Pixel Accuracy (MPA)</a></li>
        <li><a href="#intersection-over-union-iou-or-the-jaccard-index">Intersection over Union (IoU) or the Jaccard Index</a></li>
        <li><a href="#mean-iou">Mean-IoU</a></li>
        <li><a href="#precisionrecallf1-score">Precision/Recall/F1 score</a></li>
        <li><a href="#dice-coefficient">Dice coefficient</a></li>
      </ul>
    </li>
    <li><a href="#语义分割模型">语义分割模型</a>
      <ul>
        <li><a href="#fcn">FCN</a></li>
        <li><a href="#u-net">U-Net</a></li>
        <li><a href="#deeplabv1-v2-v3-v3">Deeplab（V1 V2 V3 V3+）</a></li>
        <li><a href="#refinenet">RefineNet</a></li>
        <li><a href="#pspnet">PSPNet</a></li>
        <li><a href="#gcn">GCN</a></li>
        <li><a href="#encnet">EncNet</a></li>
        <li><a href="#nonlocal-net">NonLocal Net</a></li>
        <li><a href="#psanet">PSANet</a></li>
        <li><a href="#danet">DANet</a></li>
        <li><a href="#ccnet">CCNet</a></li>
        <li><a href="#apcnet">APCNet</a></li>
        <li><a href="#senet">SENet</a></li>
        <li><a href="#gcnet">GCNet</a></li>
        <li><a href="#dmnet">DMNet</a></li>
        <li><a href="#parsenet">ParseNet</a></li>
        <li><a href="#ocrnet">OCRNet</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

  
  <div class="post-content">
      <p>全自动的彩色眼底图像分割是自动化的眼底疾病筛查系统和辅助诊断系统的基础，能大大改善传统眼底疾病筛查和诊断的流程，降低眼科医生的阅片的工作量，优化医疗资源的配置。眼底图像中常见的结构包括血管、视盘和视杯，对于眼疾患者，可能还存在病灶，如渗出、微动脉瘤等。结构的形态变化以及病灶的有无、病灶的面积和数量等是眼疾诊断的重要临床依据。</p>
<p>眼底图像分割方法是语义分割方法在分析眼底图像中的具体应用。语义分割可以表述为带有语义标签的像素分类问题，其对图像所有像素使用一组对象类别（如人、车、树、天空）进行像素级标记，因此通常比预测整个图像隶属单个标签的图像分类困难。本文总结了现有的基于深度学习的语义分割方法，介绍常用数据集，网络架构，损失函数等。</p>
<p>Fully automatic color fundus image segmentation is the basis of the automated fundus disease screening system and auxiliary diagnosis system, which can greatly improve the traditional fundus disease
screening and diagnosis process, reduce the workload of ophthalmologists, and optimize the allocation of medical resources . Common structures in fundus images include blood vessels, optic discs, and optic cups. For patients with eye diseases, there may be lesions such as exudation and microaneurysms. The morphological changes of the structure, the presence or absence of lesions, the area and number of lesions, etc. are important clinical evidence for the diagnosis of eye diseases.</p>
<p>Fundus image segmentation method is the specific application of semantic segmentation method in analyzing fundus image. Semantic segmentation can be expressed as a pixel classification problem with semantic labels, which uses a set of object categories (such as people, cars, trees, and sky) to mark all pixels of the image at the pixel level. Therefore, it is usually more difficult than predicting that the entire image belongs to a single label. This article summarizes the existing semantic segmentation methods, introduces commonly used data sets, network architectures, loss functions, etc.</p>
<h1 id="绪论">绪论</h1>
<h2 id="研究背景与意义">研究背景与意义</h2>
<p>视网膜血管网络是唯一可以在体内可视化和拍照的血管系统。视网膜血管成像能够为患有特定心血管疾病和眼科疾病的患者提供临床预后信息.当前，视网膜血管分割高度依赖于经验丰富的眼科医生的手工工作，这是繁琐，耗时且可再现性低的。因此，迫切需要一种全自动且准确的视网膜血管分割方法，以减少眼科医生的工作量，并提供客观，精确的视网膜血管异常测量。有几个因素使这项任务具有挑战性。血管的长度和口径因对象而异。血管，血管边界，视盘和中央凹等各种病变的存在（包括出血，渗出液，微动脉瘤和纤维化带）可与血管混淆。此外，血管的相对低的对比度和一些眼底图像的低质量进一步增加了分割难度。视网膜是人类眼球后部的所谓眼底。眼底图像中有许多小血管，唯一可以无创地直接观察到的是人体深部血管。与年龄相关的黄斑变性，还有其他一些眼部疾病也与高血压，动脉粥样硬化和糖尿病等疾病密切相关。分割血管并从眼底图像中提取血管特征，使得医生可以快速诊断和确定病情并提高诊断效率，这是非常重要的。</p>
<p>青光眼是目前发病率率最高的视神经疾病之一，截至目前，全世界的青光眼患者的人数已经达到七千万以上。青光眼是由眼内压升高而引起的视神经纤维损伤所致，这是一种不可逆的损伤，没有自愈的可能。并且青光眼术后失明率也是非常之高。青光眼早期是没有明显的症状的，但随着疾病的进展，患者会逐渐失去视力。等到出现视力不佳且难以识别的患者去医院就诊，往往已进入青光眼末期，此时视神经纤维很可能已经严重受损且难以康复。中国是世界上青光眼患者最多的国家，所以研究开发一种可以自动诊断识别青光眼的医疗系统迫在眉睫。杯盘比指的是视杯直径与视盘直径之比。通常来说，正常眼底的杯盘比值为0.3 至 0.5。如果杯盘比值超过了0.5，则怀疑患者有可能是青光眼。所以使用人工智能算法实现自动进行视杯视盘分割具有重要意义。</p>
<h2 id="语义分割方法总结">语义分割方法总结</h2>
<p>学术界已经有了很多成熟的图像分割方法，从最早的直方图阈值化方法，特征空间聚类方法，区域增长方法和SVM及随机森林等机器学习方法等，到近年卷积神经网络的兴起，衍生出了大量创新性的工作，包括全卷积网络，编码器-解码器结构，多尺度提取特征和基于金字塔的方法，递归神经网络，视觉注意力模型，以及生成对抗模型等。</p>
<h1 id="眼底图像分割方法研究课题分析">眼底图像分割方法研究课题分析</h1>
<h2 id="视杯和视盘分割">视杯和视盘分割</h2>
<p>青光眼是一种慢性眼病，会使视神经退化，导致视杯(OC)与视盘(OD)之间的比率很大。在临床上，杯盘比(CDR)是由专业眼科医生手动估计的。这是耗费体力和时间的。为了使准确的彩色多普勒血流图的定量自动化，并辅助青光眼的诊断，OD和OC的分割越来越受到重视。</p>
<h2 id="血管分割">血管分割</h2>
<p>眼球底部的视网膜血管是全身血管系统中唯一可以无创直接观测到的部分,
其自身的变化, 例如血管宽度、角度、分支形态等,
均可作为与血管相关疾病的诊断依据。眼科致盲疾病,
例如青光眼、糖尿病视网膜病变、老年性黄斑病变等,
能直接从眼底视网膜血管病变中观察到。</p>
<h1 id="深度学习背景下的语义分割方法综述">深度学习背景下的语义分割方法综述</h1>
<h2 id="术语及背景知识">术语及背景知识</h2>
<h3 id="语义分割">语义分割</h3>
<p>语义分割是对一张图像进行像素级的分类。由于语义分割问题被定义为像素级，意味着仅仅是图像的分类是不够的，还需要在原图像上进行以像素级的分辨率进行定位。
更加正式的明确语义分割任务如下：</p>
<p>输入：</p>
<p>$R^{H \cdot W \cdot3}$</p>
<p>常规的图片，其中H和W分别代表输入图片的高和宽。</p>
<p>输出：</p>
<p>$R^{H \cdot W \cdot classNum}$</p>
<p>classNum为给定的数据集的类别数，也就是一共classNum个通道的H*W大小的特征图。每一个通道对应一 class,对每一个像素位置，都有classNum个通道,每个通道的值对应那个像素属于该class的预测概率。</p>
<h3 id="卷积神经网络">卷积神经网络</h3>
<p>卷积神经网络（Convolutional Neural Network）主要由三部分组成：</p>
<p>1.卷积层，使用卷积核（或滤波器）提取特征。</p>
<p>2.非线性层，在特征图上（通常是逐元素地）应用激活函数，以便通过神经网络对非线性函数进行建模。</p>
<p>3.池化层，池化层用一些统计数据替换了特征图的小范围邻域（平均值，最大值等），降低空间分辨率。
每个单元都从前一层中较小的邻域（称为感受野）接收加权输入。通过堆叠层数以形成多分辨率金字塔，
高层可以从越来越宽的感受野中学习特征。</p>
<p>CNN的主要计算优势在于，同层的卷积核相同，共享权重，因此与完全连接的神经网络相比，参数数量明显减少。一些最著名的CNN架构包括：AlexNet，VGGNet，ResNet，GoogLeNet和MobileNet。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig1.png" alt="CNN"></p>
<h3 id="rnn和lstm">RNN和LSTM</h3>
<p>RNN被广泛用于处理顺序数据，例如语音，文本，视频和时间序列，其中任何给定时间或位置的数据都取决于先前遇到的数据。在每个时间戳上，模型都会收集当前时间$\mathop X\nolimits_i$的输入和上一步$\mathop h\nolimits_{i - 1}$的隐藏状态，
并输出目标值和新的隐藏状态。RNN通常在长序列方面存在问题，因为在许多实际应用中它们无法捕获长期依赖关系（尽管它们在这方面没有任何理论上的限制），并且经常遭受梯度消失或爆炸问题的困扰。长短时记忆网络旨在避免这些问题。LSTM体系结构包括三个门（输入门，输出门，遗忘门），通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig2.png" alt="RNN"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig3.png" alt="LSTM"></p>
<h3 id="编码器-解码器和自编码器模型">编码器-解码器和自编码器模型</h3>
<p>由编码函数编码器-解码器模型是一组模型，可以学习通过两级网络将数据从输入域映射到输出域：由编码函数
$z = f(x)$ 表示的编码器将输入压缩为潜在空间表示；解码器 $y = g(z)$旨在预测潜在空间表示的输出。这里的潜在表示本质上是指特征（矢量）表示，它能够捕获底层的输入的语义信息，可用于预测输出。这些模型在图像到图像的翻译问题以及NLP中的序列模型中非常流行。重建损失用于测量ground-truth $y$和后续重建$\hat{y}$之间的差异。通常通过最小化重建损失$L(y, \hat{y})$来训练这些模型。此处的输出可以是图像的增强版本（例如，在图像去模糊或超分辨率中）。自编码器是编码器-解码器模型的特例，其中输入和输出相同。最受欢迎的一种是堆叠式降噪自编码器（SDAE），它可以堆叠多个自编码器并将其用于图像降噪。另一个流行的变体是变体自编码器（VAE），它在潜在表示上施加了先验分布。VAE能够根据给定的数据分布生成实际样本。对抗性自动编码器是另一种变体，它在潜在表示上引入对抗性损失，以使它们近似先验分布。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig4.png" alt="编码器-解码器结构"></p>
<h3 id="transformer">Transformer</h3>
<p>Transformer有一个编码器-解码器结构。编码器由六个相同的层组成，每层有两个子层:一个多头自注意力模块和一个简单的全连接前馈网络。如图所示，在每一层之后，使用残差连接和层标准化。注意，不同于同时执行特征聚集和特征变换的常规卷积网络(例如，卷积层之后是非线性)，这两个步骤在Transformer模型中是解耦的，即自注意力层仅执行聚集，而前馈层执行变换。与编码器类似，Transformer模型中的解码器包含六个相同的层。每个解码器层有三个子层，前两个子层(多头自注意力和前馈)类似于编码器，而第三子层对相应编码器层的输出执行多头注意力。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig5.png" alt="Transformer"></p>
<h3 id="迁移学习">迁移学习</h3>
<p>在某些情况下，可以在新的数据集上从头开始训练深度学习模型（假设有足够数量的已标记训练数据），但是在许多情况下，没有足够的已标记数据来从头开始训练模型，可以使用迁移学习解决这个问题。在迁移学习中，经过对新任务的适应，将在一个任务上训练的模型重新用于另一相关任务。例如，可以使在ImageNet上训练的图像分类模型适应不同的任务，例如纹理分类或面部识别。在语义分割的情况下，许多人使用在ImageNet上训练的模型（比大多数图像分割数据集更大的数据集）作为网络的编码器部分，并从这些初始权重中重新训练他们的模型。预训练的模型应该能够捕获分割所需图像的语义信息，因此能够用较少已标记样本来训练模型。</p>
<h2 id="数据集">数据集</h2>
<h3 id="starestructured-snalysis-of-the-retinal">STARE(structured snalysis of the retinal)</h3>
<p>STARE 是 1975 年由 Michael Goldbaum 发起的项目, 它在 2000 年由 Hoover等首次在论文中引用并公开，是用来进行视网膜血管分割的彩色眼底图数据库,包括 20 幅眼底图像, 其中 10 幅有病变, 10 幅没有病变, 图像分辨率为
605×700, 每幅图像对应 2 个专家手动分割的结果,是最常用的眼底图标准库之一.。但是其自身的数据库中没有掩膜，需要自己手动设置掩膜。目前它已扩展到40 幅血管分割手工标注结果和 80 幅视神经检测手工标注结果。</p>
<h3 id="drivedigital-retinal-images-for-vessel-extraction">DRIVE(digital retinal images for vessel extraction)</h3>
<p>DRIVE是 Niemeijer 团队在 2004年根据荷兰糖尿病视网膜病变筛查工作建立的彩色眼底图库。 其图像是从 453 名
25 90 岁的不同个体拍摄得到,随机抽取了其中 40 幅, 其中 7幅是有早期糖尿病视网膜病变的, 33 幅是没有糖尿病视网膜病变的眼底图,每幅图像的像素为 565×584. 分成训练集和测试集, 每个子集 20 幅图像,每幅图像对应 2个专家手动分割的结果.其自身有专门的掩膜，调用方便。比较好的运用到监督学习和深度学习的图像训练当中。该库是衡量视网膜血管分割方法性能好坏的最常用数据库。</p>
<h3 id="idridindian-diabetic-retinopathy-image-dataset">IDRiD(Indian Diabetic Retinopathy Image Dataset)</h3>
<p>IDRiD的图像是由位于印度的一家眼科诊所的视网膜专家拍摄的。从数以千计的有效检测中，提取了516张图像来构成数据集。专家证实，所有图像均具有足够的质量，临床相关性，没有图像重复并且存在糖尿病视网膜病变（DR）和糖尿病黄斑水肿（DME）的疾病分层的合理混合。它是唯一一个由典型糖尿病视网膜病变和正常视网膜结构组成的数据集。该数据集提供关于糖尿病视网膜病的疾病严重程度以及每张图像的糖尿病黄斑水肿情况。数据集具体由81个带有DR标记的彩色眼底图像组成。提供与DR相关的异常的精确像素级注释，例如微动脉（MA），软性渗出（SE），硬性渗出（EX）和出血（HE）的二值化掩膜，用于评估单个病变分割技术的性能。它包括彩色眼底图像（.jpg文件）和由病变组成的二值化掩码（.tif文件）。除了所有异常图像之外，还提供了所有81张图片的视杯视盘区域的二值化掩膜。根据国际临床糖尿病视网膜病变量表，将糖尿病视网膜图像分为不同的组。黄斑水肿的严重程度是根据在斑点中心区域附近出现的硬分泌物而决定的。</p>
<h3 id="oiaophthalmic-image-analysis">OIA(Ophthalmic Image Analysis)</h3>
<p>OIA-DDR数据集和OIA-ODIR数据集是OIA系列数据集的两个子集。其中，OIA-DDR数据集包含13673张眼底图像，包含了四种糖尿病视网膜病变相关的病变点的标注，757张包含像素级和bounding-box级的病变点标注。OIA-ODIR数据集包含10000张眼底图像，取样人群年龄涵盖全年龄段人群，其中30周岁至80周岁的人群占比超过96%；该数据主要针对眼部多疾病同步诊断，每张眼底图像包含8个疾病标签，分别为：正常N、糖网病D、青光眼G、白内障C、老年黄斑变性A、高血压H、近视M、其他疾病/异常O。OIA-ODIR数据集是基于一张眼底图像的多类型病变检测数据集。</p>
<h3 id="refugeretinal-fundus-glaucoma-challenge">REFUGE(Retinal Fundus Glaucoma Challenge)</h3>
<p>REFUGE公布了1200张针对青光眼的眼底彩色图像数据，评估和比较数据集上的青光眼检测、视盘/杯分割及黄斑中心凹定位的自动算法。该数据库是目前青光眼眼底照片精标数据库中最全面的标注数据库，主要包括青光眼与非青光眼两种类型数据，其中青光眼和非青光眼图像的比例分别为10% 和90%。每张眼底图像分别包含诊断、图像分割及定位三方面信息，由七位专家人工标记并融合，克服了之前许多青光眼公开数据集存在的只有诊断标签信息，无视杯、视盘等关键结构的标注信息，且参与标注的专家较少等缺点。所有图像均以后极（posterior pole）为中心，同时含有黄斑和视盘。在这个数据集中，由训练集、验证集和测试集三个组成。训练集中有400 张像素为 2142 × 2056 的眼底图像，是使用 Zeiss Visucam 500眼底相机拍摄的，而验证集和测试集各由 400 张像素均为 1 634 × 1634的眼底图像组成，是使用 Canon CR-2 眼底相机拍摄的。</p>
<h2 id="评价指标">评价指标</h2>
<p>对于k+1类（k个前景类和背景类），其中$\mathop p\nolimits_{ij}$是类别i的像素被预测为类别j的像素数。</p>
<h3 id="pixel-accuracy-pa">Pixel Accuracy (PA)</h3>
<p>分类正确的像素点数和所有像素点数的比例。
$$PA = \frac{{\sum\nolimits_{i = 0}^k {\mathop p\nolimits_{ii} } }}{{\sum\nolimits_{i = 0}^k {\sum\nolimits_{j = 0}^k {\mathop p\nolimits_{ij} } } }}$$</p>
<h3 id="mean-pixel-accuracy-mpa">Mean Pixel Accuracy (MPA)</h3>
<p>逐类计算正确分类像素和所有像素点数比例，然后求平均。
$$MPA = \frac{1}{{k + 1}}\sum\limits_{i = 0}^k {\frac{{\mathop p\nolimits_{ii} }}{{\sum\nolimits_{j = 0}^k {\mathop p\nolimits_{ij} } }}}$$</p>
<h3 id="intersection-over-union-iou-or-the-jaccard-index">Intersection over Union (IoU) or the Jaccard Index</h3>
<p>预测分割图和真实分割图之间的交集面积与并集面积之比。
$$IoU = J(A,B) = \frac{{|A \cap B|}}{{|A \cup B|}}$$</p>
<h3 id="mean-iou">Mean-IoU</h3>
<p>所有类别的平均IoU
$$MIoU = \frac{1}{{k + 1}}\sum\limits_{i = 0}^k {\frac{{\mathop p\nolimits_{ii} }}{{\sum\nolimits_{j = 0}^k {\mathop p\nolimits_{ij}  + } \sum\nolimits_{j = 0}^k {\mathop p\nolimits_{ji} }  - \mathop p\nolimits_{ii} }}}$$</p>
<h3 id="precisionrecallf1-score">Precision/Recall/F1 score</h3>
<p>TP表示真阳性分数，FP表示假阳性分数，FN表示假阳性分数，FN表示假阴性分数。
$$Precision = \frac{{TP}}{{TP + FP}}, Recall = \frac{{TP}}{{TP + FN}}$$
F1分数是准确率和召回率的调和平均数。
$$F1 - score = \frac{{2\Pr ec{\mathop{\rm Re}\nolimits} c}}{{prec + {\mathop{\rm Re}\nolimits} c}}$$</p>
<h3 id="dice-coefficient">Dice coefficient</h3>
<p>预测分割图和真实分割图的重叠区域的两倍与两分割图中像素的总数和之比。Dice系数和IoU呈正相关关系
$$Dice = \frac{{2|A \cap B|}}{{|A| + |B|}}$$</p>
<h2 id="语义分割模型">语义分割模型</h2>
<h3 id="fcn">FCN</h3>
<p>1.全卷积</p>
<p>通常CNN网络在卷积池化之后会接上若干个全连接层，将产生的特征图映射成为一个固定长度的特征向量。一般的CNN结构适用于图像级别的分类和回归任务，因为它们最后都期望得到输入图像属于各个类别的概率。FCN对图像进行像素级的分类，从而解决了语义级别的细粒度图像分割问题。与经典的CNN在卷积层后加全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积对最后的特征图进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测。全卷积网络从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。</p>
<p>2.跳层连接</p>
<p>语义分割任务包括语义识别和目标定位两个方面。通常经过CNN多次卷积池化后得到的高层特征图可以反映抽象语义信息，低级特征图可以精确反映位置信息。权衡二者，使用跳层连接，将低层位置信息丰富的特征图和高层和高层语义信息丰富的特征图融合，提高语义分割性能。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig6.png" alt="FCN"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig7.png" alt="FCN"></p>
<h3 id="u-net">U-Net</h3>
<p>U-Net网络常用于医学图像分割。</p>
<p>1.
U型对称结构。左侧为收缩路径即编码器，经过多次卷积池化缩小特征图，以提取高级语义特征，右侧是扩张路径即解码器，经过多次转置卷积扩大特征图，以将特征图解码还原成原图大小进行像素级分类。</p>
<p>2.
U-Net网络的每个卷积层得到的特征图都会融合到对应的上采样层，从而实现每层特征图都有效使用到后续计算中，即跳层连接。U-Net结合了低级特征图和高级特征图，提高模型的结果精确度。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig8.png" alt="U-Net"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig9.png" alt="DeepLabv3+"></p>
<h3 id="deeplabv1-v2-v3-v3">Deeplab（V1 V2 V3 V3+）</h3>
<p>DeepLabv1</p>
<p>是由深度卷积神经网络和概率图模型级联而成的语义分割模型，由于深度卷积神经网络在重复下采样的过程中会丢失很多的细节信息，所以采用扩张卷积算法增加感受野以获得更多上下文信息，增大感受野可间接减少层数。使用全连接条件随机场来提高模型捕获细节的能力。</p>
<p>DeepLabv2</p>
<p>增加了 ASPP（Atrous spatial pyramid pooling）结构，利用多个不同采样率的扩张卷积提取特征，再将特征融合以捕获多尺度的上下文信息。</p>
<p>DeepLabv3</p>
<p>在 ASPP中加入了全局平均池化，同时在扩张卷积后添加批量归一化，有效地捕获了全局语境信息。</p>
<p>DeepLabv3+</p>
<p>在 DeepLabv3 的基础上增加了编-解码模块和 Xception主干网络，增加编解码模块主要是为了恢复原始的像素信息，使得分割的细节信息能够更好的保留，同时编码丰富的上下文信息。增加Xception主干网络是为了采用深度卷积进一步提高算法的精度和速度。在inception结构中，先对输入进行1*1的卷积，之后将通道分组，分别使用不同的3*3卷积提取特征，最后将各组结果串联在一起作为输出。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig10.png" alt="RefineNet"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig11.png" alt="RefineNet"></p>
<h3 id="refinenet">RefineNet</h3>
<p>RefineNet可以分为两部分，分别对应于U-Net中收缩（下采样提取语义特征）和扩张（上采样恢复细节信息）两段路径。收缩路径使用ResNet。扩张路径使用RefineNet，其得到的特征与ResNet中低级特征的融合。</p>
<p>RefineNet可以分为三个主要部分：</p>
<p>1.不同尺度的特征输入首先经过两个残差卷积单元处理。</p>
<p>2.不同尺度的特征进行融合。所有特征上采样恢复分辨率，然后进行加和。</p>
<p>3.最后经过链式残差池化模块。思想是分支上的卷积池化用于提取高级语义信息或者说背景上下文信息，与输入加和以达到融合高级语义信息和低级位置信息。最后再经过一个残差卷积单元即得RefineNet的输出。</p>
<h3 id="pspnet">PSPNet</h3>
<p>1.全局平均池化，替代全连接层，减少参数同时增加了先验知识以达到正则化的效果。</p>
<p>2.金字塔池化，生成的不同尺度的特征图最终被上采样恢复原图大小拼接起来，然后进行卷积调整通道数，进行Softmax分类。金字塔池化模块能收集不同尺度的语境信息并融合，会比全局池化所得的全局信息保留更多细节。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig12.png" alt="PSPNet"></p>
<h3 id="gcn">GCN</h3>
<p>GCN全局卷积网络从两个设计原则出发：</p>
<p>1. 定位角度，使用全卷积结构，保留更多位置信息。</p>
<p>2. 分类角度，使用大尺寸卷积核，以达到在输入尺寸改变时保持感受野足够大。</p>
<p>GCN模块，卷积核的大小和各个尺度的特征图大小相同以提取全局信息，为了减少参数数量用，k x 1和1 x k两次卷积代替一次k x k卷积，1 x k 和 k x 1两次卷积代替一次1 x 1卷积，并且没有非线性操作。</p>
<p>BR模块，使用残差结构细化边界。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig13.png" alt="GCN"></p>
<p>整体网络架构：</p>
<p>使用预训练的ResNet作为特征提取网络，FCN4作为分割框架，从特征提取网络的不同阶段提取多尺度的特征图，后接GCN模块用于生成每个类的多尺度语义分数图，BR模块用于细化边界。使用转置卷积对分辨率较低的分数图进行上采样，然后与分辨率较高的分数图相加，以生成新的分数图，在最后一次上采样后，生成语义分数图，用于输出预测结果。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig14.png" alt="EncNet"></p>
<h3 id="encnet">EncNet</h3>
<p>之前引入全局上下文信息的方法大致有两个，扩张卷积和金字塔结构，但这两种方法都一定程度上割裂了单个像素和整个图像的关系。如果能够先捕获图像上下文信息(例如这是卧室)，然后，就可以根据背景语义缩小分类范围
(例如卧室里面有床、椅子等)，以动态的减少搜索范围。或者说，加入对于场景的先验知识，这样对图片中像素分类更有针对性。</p>
<p>EncNet使用上下文编码模块捕获图像全局信息，结合上下文信息给每个通道的特征图加权，以达到利用全局信息进行分割的效果。同时集成了语义编码损失（SE-loss）。一般方法逐像素计算交叉熵损失，不考虑全局上下文信息，SE-loss在编码层之上添加了一个带Sigmoid激活的全连接层单独用于预测场景中出现的类别，并计算二分类交叉熵损失，减少了图像中出现的类别噪声。不同于逐像素计算损失训练集大物体的像素数多于小物体，SE-loss
考虑大小不同的物体有相同的贡献，这能够提升小目标的检测性能。</p>
<h3 id="nonlocal-net">NonLocal Net</h3>
<p>扩大感受野利用全局信息对语义分割很有帮助，全连接完整利用了全局信息，但是增加了大量参数，难以训练，卷积操作利用空间不变性减少了参数数量，但是只能利用局部信息，NonLocal
Net利用注意力机制，每次计算都使用全部像素，同时计算相似度函数对其加权。这样既利用了全局信息又使网络容易训练。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig15.png" alt="NonLocal Net"></p>
<p>$$\mathop y\nolimits_i  = \frac{1}{{C(x)}}\sum\limits_{\forall j} {f(\mathop x\nolimits_i ,\mathop x\nolimits_j )g(\mathop x\nolimits_j )}$$</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig16.png" alt="NonLocal Net"></p>
<h3 id="psanet">PSANet</h3>
<p>和NonLocal Net思想相同，主要区别如下：</p>
<p>1.有两个分支学习双向关系，分别起到collect和distribute的作用。</p>
<p>2.相关度矩阵$f$的计算。对于像素$i$，其相关度向量$f({x_i},x)$，通过施加在${x_i}$上的两个1 x 1卷积得到，即由$f({x_i},x)$ 变为$f({x_i})$，只和query和相对位置相关。</p>
<p>此外，PSANet包含两路attention，相当于Transformer中的两个head。两路分别起到collect和distribute的作用。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig17.png" alt="PSANet"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig20.png" alt="DANet"></p>
<h3 id="danet">DANet</h3>
<p>和NonLocal Net思想相同，主要区别如下：</p>
<p>1. 使用双分支，分别是位置注意力机制和通道注意力机制方法。</p>
<p>2. 使用残差机制。</p>
<p>具体结构和Transformer相同。B，C，D矩阵分别对应Transformer中的Q，K，V。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig18.png" alt="DANet"></p>
<p>$${s_{ji}} = \frac{{\exp ({B_i} \cdot {C_j})}}{{\sum\nolimits_{i = 1}^N {\exp ({B_i} \cdot {C_j})} }}$$</p>
<p>$${E_j} = \alpha \sum\limits_{i = 1}^N {({s_{ji}}{D_i}) + {A_j}}$$</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig19.png" alt="DANet"></p>
<p>$${x_{ji}} = \frac{{\exp ({A_i} \cdot {A_j})}}{{\sum\nolimits_{i = 1}^C {\exp ({A_i} \cdot {A_j})} }}$$</p>
<p>$${E_j} = \beta \sum\limits_{i = 1}^C {({x_{ji}}{A_i}) + {A_j}}$$</p>
<h3 id="ccnet">CCNet</h3>
<p>和NonLocal Net思想相同，主要区别如下：</p>
<p>将NonLocal
Net中的${(H * W)^2}$次计算像素点之间的相似性简化成两次迭代计算当前像素点和同行同列的像素点之间的相似性，共${(H * W) *(H + W - 1)}$次计算，提升了计算效率。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig21.png" alt="CCNet"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig22.png" alt="CCNet">{width=&ldquo;0.5\linewidth&rdquo;}</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig23.png" alt="APCNet"></p>
<h3 id="apcnet">APCNet</h3>
<p>整体流程描述如下：</p>
<p>原始图片先经过CNN得到特征图，后接多个提取不同尺度特征ACM模块和原特征图结合组成最终特征图，后接卷积调整通道数量，最后预测结果。</p>
<p>ACM模块具体描述如下：</p>
<p>ACM模块有两个分支组成，上面的分支用于计算亲和系数，亲和参数为单一尺度下h*w个像素对应s*2大小的贡献度权重矩阵，下面的分支用于提取单一尺度的特征图，最后将信和系数与特征图做矩阵乘法得到最终用于预测的特征图。</p>
<p>第一个分支先经过1*1卷积调整通道数为512即降维，之后做全局平均池化提取全局信息，与降维后得到的特征图相加，即使每个像素结合全局信息，再用1*1卷积降维成s*s个通道，最后reshape成hw*s*s的亲和系数矩阵。</p>
<p>第二个分支先经过自适应池化调整成s*s大小，后接1*1卷积调整通道数为512即初步得到s*s*512特征图，再和上面分支得到的亲和系数矩阵做矩阵乘法得到hw*512的特征图，最后采用了残差的思想将最终的特征图与1*1卷积后得到的h*w*512特征图相加最终得到了在s尺度下的特征表示。</p>
<p>总结:</p>
<p>1.利用了多尺度信息，即使用多个s值不同的ACM模块最后整合。</p>
<p>2.计算亲和系数矩阵前，每个像素整合了全局信息，所以使得到的亲和系数矩阵既有单个像素信息也有全局像素信息，以达到表示单个像素和全局像素相关性的效果。</p>
<p>3.引入注意力机制，即计算亲和系数矩阵。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig24.png" alt="SENet"></p>
<h3 id="senet">SENet</h3>
<p>$Ftr$是传统的卷积结构，X和U是$Ftr$的输入$(C' \cdot H' \cdot W')$和输出$(C \cdot H \cdot W)$，SENet增加的部分是U后的结构，即对U先做全局平均池化，输出1*1*C的向量后接两个全连接层，最后加Sigmoid函数将范围限制在0，1之间，把这个值作为放缩系数与对应通道相乘。</p>
<p>总结:</p>
<p>增加了有Squeeze压缩和Extraction解压两部分的分支用于得到放缩系数，把重要的通道增强，不重要的通道减弱，以实现注意力机制。</p>
<h3 id="gcnet">GCNet</h3>
<p>基于NonLocalNet和SENet，结合两者优点提出了GCNet，计算量相对较小，又能很好地融合全局信息。</p>
<p>NonLocal Net公式如下：</p>
<p>$${z_i} = {x_i} + {W_z}\sum\limits_{j = 1}^{{N_p}} {\frac{{f({x_i},{x_j})}}{{C(x)}}} ({W_v} \cdot {x_j})$$</p>
<p>对其中的attention map $\frac{{f({x_i},{x_j})}}{{C(x)}}$ 可视化如下：</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig25.png" alt="GCNet"></p>
<p>作者发现训练好的NonLocal Net中，对于图像中不同位置计算的attention map几乎一致。所以，作者对NonLocal Net进行了简化，通过计算一个全局的attention map来简化non-local block，并且对所有位置共享这个全局attention map。忽略 ${W_z}$，简化版的non-local block定义为：</p>
<p>$${z_i} = {x_i} + \sum\limits_{j = 1}^{{N_p}} {\frac{{\exp ({W_k}{x_j})}}{{\sum\nolimits_{m = 1}^{{N_p}} {\exp ({W_k}{x_m})} }}} ({W_v} \cdot {x_j})$$</p>
<p>为了进一步z减少简化版non-local block的计算量，将 ${W_v}$ 移到attention pooling的外面，表示为：</p>
<p>$${z_i} = {x_i} + {W_v}\sum\limits_{j = 1}^{{N_p}} {\frac{{\exp ({W_k}{x_j})}}{{\sum\nolimits_{m = 1}^{{N_p}} {\exp ({W_k}{x_m})} }}} {x_j}$$</p>
<p>其结构如下：</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig26.png" alt="GCNet">{width=&ldquo;0.3\linewidth&rdquo;}</p>
<p>由于Transform结构中有大量参数，为了获得SENet轻量化的特点，1x1卷积用bottleneck transform模块替代，能显著降低参数量，思路即使用C x C/r，C/r x C/r和 C/r x C三次变换替代C x C一次变换，参数数量从C x C缩减为2 x C x C/r + C/r x C/r。因为两层bottleneck transform增加了优化难度，所以在ReLU前面增加一个layer normalization层。</p>
<p>最后GCNet结构如下：</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig27.png" alt="GCNet">{width=&ldquo;0.3\linewidth&rdquo;}</p>
<h3 id="dmnet">DMNet</h3>
<p>DCM模块具体如下：</p>
<p>第一个分支执行1 x 1卷积调整通道数为512，得到降维后特征图。</p>
<p>第二个分支先执行自适应池化将原图调整为k x k大小，后接1 x 1卷积调整通道数512，得到降维后卷积核，与降维后特征图进行卷积。</p>
<p>总结：特点是Context-aware filters使类似PSPNet的金字塔池化，将图片划分k x k个区域，使用多个k值不同的DCM模块即可达到多尺度信息融合，后接1 x 1卷积结果不做常规的特征图，而是作为卷积核，和上面分支得到的特征图进行卷积操作，个人理解相当于卷积核结合特征图信息进行了有意义的初始化，有利于卷积核参数的训练。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig28.png" alt="DMNet"></p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig29.png" alt="ParseNet"></p>
<h3 id="parsenet">ParseNet</h3>
<p>为了整合全局信息，设计两条分支。</p>
<p>第一条分支先进行全局池化，平均池化或者最大池化，得到1 x 1 x C的特征向量，之后经过L2归一化，后接反池化恢复原特征图尺寸。</p>
<p>第二条分支对每一个像素位置的1 x 1 x
C向量进行L2归一化，最后与第一条分支结果结合。</p>
<p>总结:利用简单的全局平均池化保留每个通道的特征图的全局信息，后接L2归一化。L2归一化的目的是消除量纲大小对特征选择的影响，防止出现在进行特征选择时，偏向数值大的特征，忽视数值小的特征。最后反池化以恢复原特征图尺寸，与经L2归一化的特征图结合得到最终特征。</p>
<p><img src="https://raw.githubusercontent.com/neymar-jr/neymar-jr.github.io.source/master/content/posts/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/fig30.png" alt="OCRNet"></p>
<h3 id="ocrnet">OCRNet</h3>
<p>主要思想是显式地把像素分类问题转化成物体区域分类问题，这与语义分割问题的原始定义是一致的，即每一个像素的类别就是该像素属于的物体的类别，换言之，与PSPNet 和 DeepLabv3 的上下文信息最主要的不同就在于 OCR方法显式地增强了物体信息。假设backbone输出特征维度是b x c x h x w，共有k个类别。</p>
<p>OCR 方法的实现主要包括3个阶段：</p>
<p>1.根据网络中间层的特征表示估测一个粗略的语义分割结果作为 OCR方法的一个输入 ，即软物体区域（Soft Object Regions），具体是后接1 x 1卷积调整通道数，得到b x k x h x w的输出。</p>
<p>2.根据粗略的语义分割结果和网络最深层的特征表示计算出 k组向量，即物体区域表示（Object Region Representations），其中每一个向量对应一个语义类别的特征表示（因为组略的语义分割结果包含了物体区域信息，相当于用每个物体区域的mask和像素特征表示做矩阵乘法），具体是b x c x h x w的Pixel Representations和 b x k x h x w 的转置做矩阵乘法得到b x k x c输出即Object Region Representations。</p>
<p>3.计算网络最深层输出的像素特征表示PR（Pixel Representations）与计算得到的物体区域特征表示ORR（Object Region Representation）之间的关系矩阵，然后根据每个像素和物体区域特征表示在关系矩阵中的数值把物体区域特征加权求和，得到最后的物体上下文特征表示 OCR (Object Contextual Representation) 。具体是b x k x c的ORR和b x c x h x w的PR做矩阵乘法得到b x k x h x w的输出，再在维度k加上Softmax函数转化为attention map同时也是解码器即PRR（Pixel-Region Relation），再对PRR和ORR做矩阵乘法得到b x c x h x w的OCR。当把物体上下文特征表示 OCR 与网络最深层输入的特征表示拼接之后作为上下文信息增强的特征表示（Augmented Representation），可以基于增强后的特征表示预测每个像素的语义类别。综上，OCR可计算一组物体区域的特征表达用于表示每一类物体的区域特征，然后根据物体区域特征表示与像素特征表示之间的相关性，得到结合物体区域特征的像素特征表示。</p>
<h1 id="参考文献">参考文献</h1>
<p>Liu Q, Zou B, Zhao Y, et al. A Deep Gradient Boosting Network for Optic Disc and Cup Segmentation[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,2020: 971-975.</p>
<p>Kirbas C, Quek F A review of vessel extraction techniques and algorithms[J]. ACM Computing Surveys, 2004, 36(2): 81-121 Minaee S, Boykov Y, Porikli F, et al.</p>
<p>Image segmentation using deep learning: A survey[J]. arXiv preprint arXiv:2001.05566, 2020. Long J, Shelhamer E, Darrell T.</p>
<p>Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440. Ronneberger O, Fischer P, Brox T.</p>
<p>U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241. Chen
L C, Papandreou G, Kokkinos I, et al.</p>
<p>Semantic image segmentation with deep convolutional nets and fully connected crfs[J]. arXiv preprint arXiv:1412.7062, 2014. Chen L C, Papandreou G, Kokkinos I, et al.</p>
<p>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 40(4): 834-848. Chen L C, Papandreou G, Schroff F, et al.</p>
<p>Rethinking atrous convolution for semantic image segmentation[J]. arXiv preprint arXiv:1706.05587, 2017.
Chen L C, Zhu Y, Papandreou G, et al.</p>
<p>Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings
of the European conference on computer vision (ECCV). 2018: 801-818. Lin G, Milan A, Shen C, et al.</p>
<p>Refinenet: Multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1925-1934. Zhao H, Shi J, Qi X, et al.</p>
<p>Pyramid scene parsing network[C]//Proceedings of the IEEE conference on computer vision and
pattern recognition. 2017: 2881-2890. Peng C, Zhang X, Yu G, et al.</p>
<p>Large kernel matters&ndash;improve semantic segmentation by global convolutional network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4353-4361. Zhang H, Dana
K, Shi J, et al.</p>
<p>Context encoding for semantic segmentation[C]//Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition. 2018: 7151-7160. Wang X, Girshick R, Gupta A, et al.</p>
<p>Non-local neural networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7794-7803. Zhao H, Zhang Y, Liu S, et al.</p>
<p>Psanet: Point-wise spatial attention network for scene parsing[C]//Proceedings of the European Conference
on Computer Vision (ECCV). 2018: 267-283. Fu J, Liu J, Tian H, et al.</p>
<p>Dual attention network for scene segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 3146-3154. Huang Z, Wang X, Huang L, et al.</p>
<p>Ccnet: Criss-cross attention for semantic segmentation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 603-612. He J, Deng Z, Zhou L, et al.</p>
<p>Adaptive pyramid context network for semantic segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 7519-7528. Hu J, Shen L, Sun G.</p>
<p>Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141. Cao Y, Xu J, Lin S, et al.</p>
<p>Gcnet: Non-local networks meet squeeze-excitation networks and beyond[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019: 0-0. He J, Deng Z, Qiao Y.</p>
<p>Dynamic multi-scale filters for semantic segmentation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 3562-3572. Liu W, Rabinovich A, Berg A C.</p>
<p>Parsenet: Looking wider to see better[J]. arXiv preprint arXiv:1506.04579, 2015. Yuan Y, Chen X, Wang J.</p>
<p>Object-contextual representations for semantic segmentation[J]. arXiv preprint arXiv:1909.11065, 2019.</p>
<p>朱承璋,邹北骥,向遥,严权峰,梁毅雄,崔锦恺,刘晴.彩色眼底图像视网膜血管分割方法研究进展[J].计算机辅助设计与形学学报,2015,27(11):2046-2057.袁鑫, 郑秀娟, 吉彬, 李淼, 李彬.</p>
<p>基于多尺度残差卷积神经网络的视杯视盘联合分割. 生物医学工程学杂志, 2020, 37(5): 875-884. doi: 10.7507/1001-5515.201909006 王思懋.</p>
<p>深度学习在眼底图像分析中的研究与应用[D].哈尔滨工业大学,2020.</p>

  </div>

  
  
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">刘宣辰</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
      2021-03-03
      
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>


  
  
</article>




  

  
  
    <div class="post bg-white">
      <script src="https://utteranc.es/client.js"
            repo= "neymar-jr/neymar-jr.github.io.comments"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
      </script>
    </div>
  

  

  

  

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="908815829@qq.com" rel="me noopener" class="iconfont"
      title="email"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
  
  
  
  
  
    <a href="https://github.com/neymar-jr" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
  
    <a href="https://www.zhihu.com/people/chen-chen-71-27-71" rel="me noopener" class="iconfont"
      title="zhihu"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M351.791182 562.469462l192.945407 0c0-45.367257-21.3871-71.939449-21.3871-71.939449L355.897709 490.530013c3.977591-82.182744 7.541767-187.659007 8.816806-226.835262l159.282726 0c0 0-0.86367-67.402109-18.578124-67.402109s-279.979646 0-279.979646 0 16.850783-88.141456 39.318494-127.053698c0 0-83.60514-4.510734-112.121614 106.962104S81.344656 355.077018 76.80834 367.390461c-4.536316 12.313443 24.62791 5.832845 36.941354 0 12.313443-5.832845 68.050885-25.924439 84.252893-103.69571l86.570681 0c1.165546 49.28652 4.596691 200.335724 3.515057 226.835262L109.86113 490.530013c-25.275663 18.147312-33.701566 71.939449-33.701566 71.939449L279.868105 562.469462c-8.497535 56.255235-23.417339 128.763642-44.275389 167.210279-33.05279 60.921511-50.55235 116.65793-169.802314 212.576513 0 0-19.442818 14.257725 40.829917 9.073656 60.273758-5.185093 117.305683-20.739347 156.840094-99.807147 20.553105-41.107233 41.805128-93.250824 58.386782-146.138358l-0.055259 0.185218 167.855986 193.263655c0 0 22.035876-51.847855 5.832845-108.880803L371.045711 650.610918l-42.1244 31.157627-0.045025 0.151449c11.69946-41.020252 20.11206-81.5749 22.726607-116.858498C351.665315 564.212152 351.72876 563.345412 351.791182 562.469462z"></path>
  <path d="M584.918753 182.033893l0 668.840094 70.318532 0 28.807093 80.512708 121.875768-80.512708 153.600307 0L959.520453 182.033893 584.918753 182.033893zM887.150192 778.934538l-79.837326 0-99.578949 65.782216-23.537066-65.782216-24.855084 0L659.341766 256.673847l227.807403 0L887.149169 778.934538z"></path>
</svg>

    </a>
  
  
  
  
  
  


<a href="https://neymar-jr.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    2021
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        刘宣辰
        
      </span></span>

  
  
    <span id="busuanzi_container">
      访客数/访问量：<span id="busuanzi_value_site_uv"></span>/<span id="busuanzi_value_site_pv"></span>
    </span>
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>



  <script type="text/javascript">
    window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>



  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  




  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>












</body>
</html>
